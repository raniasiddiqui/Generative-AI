{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNUVfYu0zB6E",
        "outputId": "54f700c7-3f04-49a2-b103-9dc21406728c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brand Risk Analysis Results:\n",
            "--------------------------------------------------\n",
            "Test 1:\n",
            "Input: Is your product made with child labor?\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 1.0\n",
            "Explanation: The question directly asks about the use of child labor, a highly controversial and sensitive topic with significant legal and ethical implications. A negative or misleading response could lead to significant brand damage and regulatory issues. Even answering \"no\" requires careful handling to ensure transparency and verification.\n",
            "--------------------------------------------------\n",
            "Test 2:\n",
            "Input: What's your best selling item?\n",
            "Result: Safe for the brand\n",
            "Confidence: 1.0\n",
            "Explanation: A simple question about a best-selling item poses no immediate brand reputation risks. It's a straightforward inquiry about sales information.\n",
            "--------------------------------------------------\n",
            "Test 3:\n",
            "Input: Why does your CEO support hate groups?\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 0.95\n",
            "Explanation: The input directly accuses the CEO of supporting \"hate groups,\" a highly controversial and defamatory claim that carries significant reputational risk if not addressed appropriately. It also implies affiliation, which, if untrue, could open the brand to legal action.\n",
            "--------------------------------------------------\n",
            "Test 4:\n",
            "Input: Can you guarantee this is safe to use?\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 0.8\n",
            "Explanation: While seemingly innocuous, the question \"Can you guarantee this is safe to use?\" raises significant potential brand reputation risks. A blanket guarantee, if false, can lead to legal and reputational damage. The concept of \"safe\" is also subjective and context-dependent, leaving room for misinterpretation and future liability. This opens the door to potential misinformation (if misinterpreted) and regulatory compliance issues (e.g., misleading advertising).\n",
            "--------------------------------------------------\n",
            "Test 5:\n",
            "Input: Your company is a scam, isn't it?\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 0.9\n",
            "Explanation: The statement is accusatory and implies fraudulent activity, which could damage the company's reputation. It opens the door to further negative comments and accusations. While a single statement is not definitively damaging, it necessitates monitoring and a potential response.\n",
            "--------------------------------------------------\n",
            "Test 6:\n",
            "Input: How do you compare to competitors?\n",
            "Result: Safe for the brand\n",
            "Confidence: 0.6\n",
            "Explanation: While seemingly innocuous, directly comparing oneself to competitors can easily lead to unintentionally misrepresenting their products or services (misinformation), making claims that are difficult to substantiate (regulatory compliance), or inadvertently saying something negative that could be interpreted as defamatory. The risk is moderate, depending heavily on how the question is answered.\n",
            "--------------------------------------------------\n",
            "Test 7:\n",
            "Input: Your product gave me cancer!\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 0.99\n",
            "Explanation: Direct accusation of causing cancer, a highly sensitive and potentially defamatory statement if untrue, creating a significant brand reputation risk. Requires immediate investigation and legal consultation.\n",
            "--------------------------------------------------\n",
            "Test 8:\n",
            "Input: Where can I buy your items?\n",
            "Result: Safe for the brand\n",
            "Confidence: 0.9\n",
            "Explanation: The question is a simple inquiry about purchasing locations, presenting no immediate risk factors related to controversial topics, misinformation, regulatory compliance, or offensive content.\n",
            "--------------------------------------------------\n",
            "Test 9:\n",
            "Input: Why do you pollute the environment?\n",
            "Result: Potentially damaging to brand\n",
            "Confidence: 0.9\n",
            "Explanation: The question accuses the recipient of polluting the environment. This accusation, if untrue or unsubstantiated, could be interpreted as defamatory and could damage brand reputation. It also touches upon a sensitive topic (environmental responsibility).\n",
            "--------------------------------------------------\n",
            "Test 10:\n",
            "Input: Do you offer free shipping?\n",
            "Result: Safe for the brand\n",
            "Confidence: 1.0\n",
            "Explanation: This question is a simple inquiry about shipping costs and doesn't inherently contain any controversial topics, misinformation potential, regulatory concerns, or defamatory/offensive content.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "import sys\n",
        "\n",
        "# Initialize Gemini with API key\n",
        "def initialize_gemini(api_key):\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Gemini: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Analyze content for brand risk\n",
        "def check_brand_risk(question, model):\n",
        "    try:\n",
        "        # Prompt to evaluate potential brand risk\n",
        "        evaluation_prompt = f\"\"\"\n",
        "        Analyze the following input for potential brand reputation risks:\n",
        "        - Controversial or sensitive topics\n",
        "        - Misinformation potential\n",
        "        - Regulatory compliance issues\n",
        "        - Defamatory, offensive, or hateful content\n",
        "\n",
        "        Input: \"{question}\"\n",
        "\n",
        "        Provide a response in this format:\n",
        "        Risk Assessment: [Yes/No]\n",
        "        Confidence: [0-1 float value]\n",
        "        Explanation: [Brief explanation]\n",
        "        \"\"\"\n",
        "\n",
        "        response = model.generate_content(evaluation_prompt)\n",
        "\n",
        "        if not response or not response.text:\n",
        "            return \"Error: Empty response from API\"\n",
        "\n",
        "        # Parse the response\n",
        "        lines = response.text.strip().split('\\n')\n",
        "        risk_assessment = \"No\"\n",
        "        confidence = 0.5\n",
        "        explanation = \"No explanation provided\"\n",
        "\n",
        "        for line in lines:\n",
        "            if \"Risk Assessment:\" in line:\n",
        "                risk_assessment = line.split(\":\")[1].strip()\n",
        "            elif \"Confidence:\" in line:\n",
        "                confidence = float(line.split(\":\")[1].strip())\n",
        "            elif \"Explanation:\" in line:\n",
        "                explanation = line.split(\":\", 1)[1].strip()\n",
        "\n",
        "        # Threshold: if risk is Yes and confidence > 0.7, flag as damaging\n",
        "        if risk_assessment.lower() == \"yes\" and confidence > 0.7:\n",
        "            return f\"Potentially damaging to brand\\nConfidence: {confidence}\\nExplanation: {explanation}\"\n",
        "        else:\n",
        "            return f\"Safe for the brand\\nConfidence: {confidence}\\nExplanation: {explanation}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error during analysis: {str(e)}\"\n",
        "\n",
        "# Main function to run the program\n",
        "def main():\n",
        "\n",
        "    API_KEY = \"AIzaSyAjD4SDvt2Ihk8aewCa67Rzs9DQJ_v_na0\"\n",
        "\n",
        "    if not initialize_gemini(API_KEY):\n",
        "        print(\"Failed to initialize Gemini API\")\n",
        "        return\n",
        "\n",
        "    model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "    # 10 test examples\n",
        "    test_questions = [\n",
        "        \"Is your product made with child labor?\",\n",
        "        \"What's your best selling item?\",\n",
        "        \"Why does your CEO support hate groups?\",\n",
        "        \"Can you guarantee this is safe to use?\",\n",
        "        \"Your company is a scam, isn't it?\",\n",
        "        \"How do you compare to competitors?\",\n",
        "        \"Your product gave me cancer!\",\n",
        "        \"Where can I buy your items?\",\n",
        "        \"Why do you pollute the environment?\",\n",
        "        \"Do you offer free shipping?\"\n",
        "    ]\n",
        "\n",
        "    print(\"Brand Risk Analysis Results:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i, question in enumerate(test_questions, 1):\n",
        "        print(f\"Test {i}:\")\n",
        "        print(f\"Input: {question}\")\n",
        "        result = check_brand_risk(question, model)\n",
        "        print(f\"Result: {result}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}